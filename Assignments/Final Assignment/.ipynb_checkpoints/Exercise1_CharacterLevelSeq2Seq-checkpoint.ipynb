{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d74f65-0f40-472f-a8a3-6637a5103574",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "## Character-level recurrent sequence-to-sequence model\n",
    "### By: Daniel Mehta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd480e8f-58d7-45e3-8595-41501657d44f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9fa93b-0b31-4452-9a4d-eabd6bf0a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b804f7-a741-499f-aff0-75c0e7665e3e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895960e1-941f-48fd-bdbb-62d2545b8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting path to dataset\n",
    "data_dir = Path(\"fra-eng\")\n",
    "data_path = data_dir/\"fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ef6fba-3d0d-495b-8e1e-f2da1ae665ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset located at: fra-eng\\fra.txt\n"
     ]
    }
   ],
   "source": [
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {data_path}\")\n",
    "\n",
    "print(f\"Dataset located at: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770d912-8a17-4fd4-adcf-30373b0260ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Exploration and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d88e59-16c9-40e4-b762-5fb350b61919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file and split into lines\n",
    "with open(data_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f60c51ae-6f7b-462c-b125-43ef68763452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence pairs in file: 237838\n",
      "Sample lines:\n",
      "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
      "Go.\tMarche.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)\n",
      "Go.\tEn route !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)\n",
      "Go.\tBouge !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)\n",
      "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total sentence pairs in file: {len(lines)}\")\n",
    "print(\"Sample lines:\")\n",
    "for i in range(5):\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68341922-7148-4bac-a3e7-027524f41cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating into English and French\n",
    "pairs =[line.split(\"\\t\") for line in lines]\n",
    "english_sentences =[pair[0] for pair in pairs]\n",
    "french_sentences =[pair[1] for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1045d875-c896-4d97-9167-4705e10e060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example pair:\n",
      "EN: Go.\n",
      "FR: Va !\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample pair:\")\n",
    "print(\"EN:\",english_sentences[0])\n",
    "print(\"FR:\",french_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25bcd2e-93b9-448c-9d70-b724f6bf63f5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13eb557-9067-415f-b9ed-c54f776033b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up seed\n",
    "SEED = 5501\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c72848-39b2-420f-aaa2-bc61dcf13be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settubg yo start and end tokens\n",
    "START_TOKEN=\"\\t\"\n",
    "END_TOKEN=\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aec7b844-6f15-4278-8430-0696a6887152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=64, epochs=100, latent_dim=256, num_samples=10000\n",
      "Decoder tokens -> start: '\\t', end: '\\n'\n"
     ]
    }
   ],
   "source": [
    "#  hyperparameters\n",
    "batch_size = 64 # Batch size\n",
    "epochs =100 # epochs of training\n",
    "latent_dim = 256 #Latent dimensionality of the encoding space\n",
    "num_samples = 10000  # Num of samples\n",
    "\n",
    "print(f\"batch_size={batch_size}, epochs={epochs}, latent_dim={latent_dim}, num_samples={num_samples}\")\n",
    "print(f\"Decoder tokens -> start: {repr(START_TOKEN)}, end: {repr(END_TOKEN)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41096a96-875c-4369-b574-27a4c3dd9894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"PyTorch version:\",torch.__version__)\n",
    "print(\"CUDA available:\",torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "262344ce-634f-4e78-bef3-0d82c0114f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 90\n",
      "Target vocab size: 113\n"
     ]
    }
   ],
   "source": [
    "# Building vocabularies\n",
    "\n",
    "#  sorted unique characters for each language\n",
    "input_characters = sorted(list(set(\"\".join(english_sentences))))\n",
    "target_characters = sorted(list(set(\"\".join(french_sentences))))\n",
    "\n",
    "#mapping dicts\n",
    "input_char_to_idx ={char: idx for idx, char in enumerate(input_characters)}\n",
    "input_idx_to_char ={idx: char for char, idx in input_char_to_idx.items()}\n",
    "\n",
    "target_char_to_idx ={char: idx for idx, char in enumerate(target_characters)}\n",
    "target_idx_to_char ={idx: char for char, idx in target_char_to_idx.items()}\n",
    "\n",
    "# vocabulary sizes\n",
    "input_vocab_size = len(input_characters)\n",
    "target_vocab_size = len(target_characters)\n",
    "\n",
    "print(f\"Input vocab size: {input_vocab_size}\")\n",
    "print(f\"Target vocab size: {target_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "319b986c-a6ce-4e6c-ab65-3f41aed86952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start/end tokens to french\n",
    "french_sentences = [START_TOKEN+s+END_TOKEN for s in french_sentences]\n",
    "\n",
    "# Limit samples\n",
    "english_sentences = english_sentences[:num_samples]\n",
    "french_sentences = french_sentences[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34e37714-97ce-47b0-87bc-798ed01f9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the sorted unique characters\n",
    "input_characters = sorted(list(set(\"\".join(english_sentences))))\n",
    "target_characters = sorted(list(set(\"\".join(french_sentences))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dad5c8e-1a50-46f0-a137-bbf388b5da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map char to index\n",
    "input_char_to_idx = {char: idx for idx,char in enumerate(input_characters)}\n",
    "input_idx_to_char = {idx: char for char,idx in input_char_to_idx.items()}\n",
    "\n",
    "target_char_to_idx = {char: idx for idx,char in enumerate(target_characters)}\n",
    "target_idx_to_char = {idx: char for char,idx in target_char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d666d5f1-bee5-40f6-aab1-1e3ba1b38770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocab size: 70\n",
      "Target vocab size: 91\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary sizes\n",
    "input_vocab_size =len(input_characters)\n",
    "target_vocab_size =len(target_characters)\n",
    "\n",
    "print(f\"Input vocab size: {input_vocab_size}\")\n",
    "print(f\"Target vocab size: {target_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab7c5d09-7571-4f4d-8677-e73d18f96b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches in train_dataloader: 157\n"
     ]
    }
   ],
   "source": [
    "# Convertting to index tensors\n",
    "src_tensors = [torch.tensor([input_char_to_idx[ch] for ch in s],dtype=torch.long) \n",
    "               for s in english_sentences]\n",
    "tgt_input_tensors = [torch.tensor([target_char_to_idx[ch] for ch in s[:-1]],dtype=torch.long) \n",
    "                     for s in french_sentences]\n",
    "tgt_target_tensors = [torch.tensor([target_char_to_idx[ch] for ch in s[1:]],dtype=torch.long) \n",
    "                      for s in french_sentences]\n",
    "\n",
    "# Pad sequences, 0 will be pad index\n",
    "src_tensors=pad_sequence(src_tensors, batch_first=True,padding_value=0)\n",
    "tgt_input_tensors=pad_sequence(tgt_input_tensors, batch_first=True,padding_value=0)\n",
    "tgt_target_tensors=pad_sequence(tgt_target_tensors, batch_first=True,padding_value=0)\n",
    "\n",
    "# Create Dataloader\n",
    "dataset = list(zip(src_tensors,tgt_input_tensors,tgt_target_tensors))\n",
    "train_dataloader =DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "print(f\"Batches in train_dataloader: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3316d23-94cf-4913-85f4-78e47f300b8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "710773a4-60ec-46b8-84c9-da0c8ce56644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(70, 128)\n",
      "    (lstm): LSTM(128, 256, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(91, 128)\n",
      "    (lstm): LSTM(128, 256, batch_first=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=91, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128 # it must be smaller or equal to the latent dim\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, embed_dim,latent_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=latent_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, src_idxs):\n",
    "        # src_idxs:(batch, src_len)\n",
    "        embedded =self.embedding(src_idxs) # (batch, src_len, embed_dim)\n",
    "        outputs,(h,c) =self.lstm(embedded) #outputs not used,keep states\n",
    "        return h,c\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim,latent_dim):\n",
    "        super().__init__()\n",
    "        self.embedding =nn.Embedding(target_vocab_size,embed_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=latent_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out =nn.Linear(latent_dim,target_vocab_size)\n",
    "        \n",
    "    def forward(self, tgt_idxs, hidden, cell):\n",
    "        # tgt_idxs:(batch, tgt_len) with teacher forcing\n",
    "        embedded = self.embedding(tgt_idxs)# (batch, tgt_len,embed_dim)\n",
    "        outputs, (h,c) = self.lstm(embedded, (hidden,cell))\n",
    "        logits = self.fc_out(outputs)#(batch, tgt_len, target_vocab_size)\n",
    "        return logits, h,c\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder =encoder\n",
    "        self.decoder =decoder\n",
    "\n",
    "    def forward(self, src_idxs, tgt_input_idxs):\n",
    "        # training forward pass with teacher forcing\n",
    "        h,c =self.encoder(src_idxs)\n",
    "        logits,_,_=self.decoder(tgt_input_idxs, h,c)\n",
    "        return logits\n",
    "\n",
    "\n",
    "#Instantiate and move to device\n",
    "encoder = Encoder(input_vocab_size, embed_dim, latent_dim)\n",
    "decoder = Decoder(target_vocab_size, embed_dim, latent_dim)\n",
    "model =Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "PAD_IDX =None\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) if PAD_IDX is not None else nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662bfb7f-38db-402b-9f27-766defa816f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156af48-8467-4678-b303-71c235608247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████████████████████████████████████████████████████████████| 157/157 [00:01<00:00, 130.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.8973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████████████████████████████████████████████████████████████| 157/157 [00:00<00:00, 193.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.5402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████████████████████████████████████████████████████████████| 157/157 [00:00<00:00, 197.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.4538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████████████████████████████████████████████████████████████| 157/157 [00:00<00:00, 196.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.4037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████████████████████████████████████████████████████████████| 157/157 [00:00<00:00, 195.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.3691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████████████████████████████████████████████████████████████| 157/157 [00:00<00:00, 196.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 0.3401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100:  13%|████████▌                                                          | 20/157 [00:00<00:00, 197.02it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+ 1):\n",
    "    model.train()\n",
    "    total_loss =0\n",
    "\n",
    "    for src, tgt_input, tgt_target in tqdm(train_dataloader,desc=f\"Epoch {epoch}/{epochs}\"):\n",
    "        src = src.to(device)\n",
    "        tgt_input =tgt_input.to(device)\n",
    "        tgt_target =tgt_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output_logits = model(src, tgt_input) #(batch, tgt_len, vocab_size)\n",
    "\n",
    "        # Reshape for loss, mergeing batch & time dims\n",
    "        output_logits = output_logits.reshape(-1,target_vocab_size)\n",
    "        tgt_target = tgt_target.reshape(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output_logits,tgt_target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "    avg_loss = total_loss/len(train_dataloader)\n",
    "    print(f\"Epoch {epoch} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(),\"seq2seq_model.pth\")\n",
    "print(\"Model saved to seq2seq_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
