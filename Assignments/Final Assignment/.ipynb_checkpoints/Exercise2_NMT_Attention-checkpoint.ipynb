{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2958f5c-89d8-449c-b0b8-c85abdcc6a30",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "## Neural machine translation with attention\n",
    "### By: Daniel Mehta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b1a81-38b7-44b4-af2c-01d7245cbebb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8752830-c3d1-4b09-b820-bfdf8b5496f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73e25d9-56aa-4021-8aa3-0ba43834733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up seed\n",
    "SEED = 5501\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bed902-4410-45db-ba9b-c42d046bbbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# setting device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76489bd6-9d9d-4bf3-938b-13096d5fee09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc156b12-e9ba-4e8f-a908-205009784407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting path to dataset\n",
    "data_dir = Path(\"spa-eng\")\n",
    "data_path = data_dir / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780ef227-87f6-4cbc-9ced-e546cb7ce148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset located at: spa-eng\\spa.txt\n"
     ]
    }
   ],
   "source": [
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {data_path}\")\n",
    "print(f\"Dataset located at: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af606eb-07c1-447d-b2c8-1df3b1275a22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Exploration and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d6e9ba-1918-4402-9589-c542bafa350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file and split into lines\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ca9c00-a792-497c-8576-d190c043702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence pairs in file: 142511\n",
      "Sample lines:\n",
      "Go.\tVe.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986655 (cueyayotl)\n",
      "Go.\tVete.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986656 (cueyayotl)\n",
      "Go.\tVaya.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #4986657 (cueyayotl)\n",
      "Go.\tVÃ¡yase.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #6586271 (arh)\n",
      "Hi.\tHola.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #431975 (Leono)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total sentence pairs in file: {len(lines)}\")\n",
    "print(\"Sample lines:\")\n",
    "for i in range(5):\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15a78ea1-9da5-48cb-b27c-b5922ac18de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example pair:\n",
      "EN: Go.\n",
      "ES: Ve.\n"
     ]
    }
   ],
   "source": [
    "#Separating into English and Spanish\n",
    "pairs = [line.split(\"\\t\") for line in lines]\n",
    "english_sentences = [pair[0] for pair in pairs] #English (target)\n",
    "spanish_sentences = [pair[1] for pair in pairs] #Spanish (source)\n",
    "\n",
    "print(\"\\nExample pair:\")\n",
    "print(\"EN:\", english_sentences[0])\n",
    "print(\"ES:\", spanish_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd0046-cdd1-4b98-8de2-ec3bdc5a325d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tokenization & vocab building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d1b6e5-4478-47b9-8f8a-822b6bb378b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  start and end tokens to the English targets\n",
    "START_TOKEN=\"<start>\"\n",
    "END_TOKEN=\"<end>\"\n",
    "\n",
    "english_sentences = [f\"{START_TOKEN} {s} {END_TOKEN}\" for s in english_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6920bffb-3949-4bd0-bda1-a45450c157f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic tokenization\n",
    "#lowercase, split on spaces, strip punctuation\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b14735e-2123-445b-b2aa-fc9b4d60d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize all the sentences\n",
    "tokenized_es = [tokenize(s) for s in spanish_sentences]\n",
    "tokenized_en = [tokenize(s) for s in english_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37b8005c-b417-40b6-a883-415fc276e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build vocabularies\n",
    "def build_vocab(tokenized_sents,min_freq=1):\n",
    "    counter = Counter(token for sent in tokenized_sents for token in sent)\n",
    "    vocab = {token: idx+2 for idx, (token, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab[\"<pad>\"] =0\n",
    "    vocab[\"<unk>\"] =1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4be4cd2-8bf2-4384-927b-28a7b321d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = build_vocab(tokenized_es)\n",
    "tgt_vocab = build_vocab(tokenized_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a91776-70ad-400f-af7f-120512f34068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse look up for decoding \n",
    "src_idx2word = {idx: word for word,idx in src_vocab.items()}\n",
    "tgt_idx2word = {idx: word for word,idx in tgt_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e2988be-01e3-4e6c-93b5-96429e0e19d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab size (ES): 46045\n",
      "Target vocab size (EN): 25767\n"
     ]
    }
   ],
   "source": [
    "print(f\"Source vocab size (ES): {len(src_vocab)}\")\n",
    "print(f\"Target vocab size (EN): {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d504f-af82-49ae-abd6-80da48c7f706",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Convert sentences to index tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c60cd8-66c1-48b6-b298-e180c4006ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numericalize tokenized sentences\n",
    "def numericalize(tokenized_sents,vocab):\n",
    "    return [torch.tensor([vocab.get(tok,vocab[\"<unk>\"]) for tok in sent],dtype=torch.long) \n",
    "            for sent in tokenized_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b77b5e-0da4-40bc-9b8d-3ea4ae4d1e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tensors =numericalize(tokenized_es, src_vocab)\n",
    "tgt_input_tensors = numericalize([sent[:-1] for sent in tokenized_en], tgt_vocab)# without <end>\n",
    "tgt_target_tensors = numericalize([sent[1:] for sent in tokenized_en], tgt_vocab)# without <start>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "989f45f6-1711-41e3-830f-a84c906ceea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "src_tensors = pad_sequence(src_tensors,batch_first=True, padding_value=src_vocab[\"<pad>\"])\n",
    "tgt_input_tensors = pad_sequence(tgt_input_tensors,batch_first=True, padding_value=tgt_vocab[\"<pad>\"])\n",
    "tgt_target_tensors = pad_sequence(tgt_target_tensors,batch_first=True, padding_value=tgt_vocab[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "856ecd83-dfda-46ef-951f-8ff32a7b3bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example ES tensor: tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Example EN input tensor: tensor([2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Example EN target tensor: tensor([3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example ES tensor: {src_tensors[0]}\")\n",
    "print(f\"Example EN input tensor: {tgt_input_tensors[0]}\")\n",
    "print(f\"Example EN target tensor: {tgt_target_tensors[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c9077-f0ea-403a-ae9f-59ba4cbe48b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataloader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a67358cc-2ea9-4712-a6e8-79096984b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combineing into dataset\n",
    "dataset=list(zip(src_tensors,tgt_input_tensors,tgt_target_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26bba207-e31e-40dc-a4d5-a4b1ea3a976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 1782, Val batches: 446\n"
     ]
    }
   ],
   "source": [
    "# Train/validation split (80/20)\n",
    "split_idx=int(len(dataset)*0.8)\n",
    "train_data=dataset[:split_idx]\n",
    "val_data=dataset[split_idx:]\n",
    "\n",
    "BATCH_SIZE =64\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_loader = DataLoader(val_data,batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68f4c4-aaff-4ca9-b405-beb9b5cf691c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "148cbeff-0d97-49e6-b3cc-ffb3878f4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim) # project bi GRU output to hidden_dim\n",
    "\n",
    "    def forward(self, src_idxs):\n",
    "        # src_idxs: (batch, src_len)\n",
    "        embedded = self.embedding(src_idxs) # (batch, src_len, embed_dim)\n",
    "        outputs, hidden = self.gru(embedded)# outputs: (batch, src_len, hidden_dim*2)\n",
    "        \n",
    "        # Mergeing the bidirectional hidden states\n",
    "        hidden =torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))) # (batch,hidden_dim)\n",
    "        hidden =hidden.unsqueeze(0) # (1, batch, hidden_dim)\n",
    "        \n",
    "        return outputs,hidden #outputs for attention, hidden for decoder init\n",
    "\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_dim, dec_dim) #project encoder outputs to decoder dim\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs, src_mask=None):\n",
    "        # decoder_hidden: (1, batch, dec_dim)\n",
    "        # encoder_outputs: (batch, src_len, enc_dim)\n",
    "        # src_mask: (batch, src_len) -> 1 for real tokens, 0 for PAD\n",
    "        \n",
    "        # Project encoder outputs to decoder hidden size\n",
    "        proj_enc = self.attn(encoder_outputs) #(batch, src_len, dec_dim)\n",
    "        \n",
    "        # Repeat decoder hidden state across src_len\n",
    "        decoder_hidden = decoder_hidden.permute(1, 0, 2) #(batch, 1, dec_dim)\n",
    "        \n",
    "        # score: batch matrix multiply\n",
    "        scores = torch.bmm(proj_enc, decoder_hidden.transpose(1, 2)) #(batch, src_len, 1)\n",
    "\n",
    "        if src_mask is not None:\n",
    "            # mask pad positions before softmax\n",
    "            scores = scores.masked_fill(src_mask.unsqueeze(2) == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=1) #(batch, src_len, 1)\n",
    "        \n",
    "        # Context vector\n",
    "        context = torch.bmm(attn_weights.transpose(1, 2), encoder_outputs) #(batch, 1, enc_dim)\n",
    "        \n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_vocab_size, embed_dim,hidden_dim, enc_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_vocab_size, embed_dim)\n",
    "        self.attention = LuongAttention(enc_dim,hidden_dim)\n",
    "        self.ctx_proj = nn.Linear(enc_dim,hidden_dim) #project context to decoder dim\n",
    "        self.gru = nn.GRU(embed_dim+hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim*2,output_vocab_size)\n",
    "\n",
    "    def forward(self,tgt_input_idxs, hidden, encoder_outputs):\n",
    "        # tgt_input_idxs:(batch, tgt_len)\n",
    "        embedded = self.embedding(tgt_input_idxs) #(batch,tgt_len,embed_dim)\n",
    "        \n",
    "        outputs=[]\n",
    "        for t in range(embedded.size(1)): # step through target sequence\n",
    "            input_t = embedded[:,t,:].unsqueeze(1) #(batch,1,embed_dim)\n",
    "            \n",
    "            # Attention context\n",
    "            context, attn_weights = self.attention(hidden, encoder_outputs) # context:(batch,1,enc_dim)\n",
    "            ctx_dec = self.ctx_proj(context) #(batch,1,hidden_dim)\n",
    "            \n",
    "            #Combine context with current input\n",
    "            rnn_input = torch.cat((input_t, ctx_dec),dim=2) #(batch,1,embed_dim+hidden_dim)\n",
    "            \n",
    "            output, hidden = self.gru(rnn_input, hidden) # output:(batch,1,hidden_dim)\n",
    "            \n",
    "            # final output layer\n",
    "            output_combined = torch.cat((output, ctx_dec),dim=2) #(batch,1,hidden_dim*2)\n",
    "            prediction = self.fc_out(output_combined) #(batch,1,output_vocab_size)\n",
    "            \n",
    "            outputs.append(prediction)\n",
    "        \n",
    "        outputs = torch.cat(outputs,dim=1) #(batch,tgt_len,output_vocab_size)\n",
    "        return outputs\n",
    "\n",
    "    def step(self,input_token_idxs, hidden, encoder_outputs):\n",
    "        # input_token_idxs:(batch,1)\n",
    "        embedded = self.embedding(input_token_idxs) #(batch,1,embed_dim)\n",
    "        \n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs) #(batch,1,enc_dim)\n",
    "        ctx_dec = self.ctx_proj(context) #(batch,1,hidden_dim)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, ctx_dec),dim=2) #(batch,1,embed_dim+hidden_dim)\n",
    "        \n",
    "        output, hidden = self.gru(rnn_input, hidden) #(batch,1,hidden_dim)\n",
    "        \n",
    "        output_combined = torch.cat((output, ctx_dec),dim=2) #(batch,1,hidden_dim*2)\n",
    "        prediction = self.fc_out(output_combined) #(batch,1,output_vocab_size)\n",
    "        \n",
    "        return prediction, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src_idxs, tgt_input_idxs):\n",
    "        encoder_outputs, hidden = self.encoder(src_idxs)\n",
    "        outputs = self.decoder(tgt_input_idxs, hidden, encoder_outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a7423-7f28-4fc5-aa7f-45028cc984de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Config and Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20ec2bec-629f-484e-a385-26faa8d3e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBED_DIM = 256 # size of word embeddings\n",
    "HIDDEN_DIM = 256 # size of GRU hidden state\n",
    "BATCH_SIZE = 64 # sentence per batch\n",
    "EPOCHS = 15 # training passes over dataset\n",
    "LR = 0.001 # learning rate\n",
    "\n",
    "PAD_SRC = src_vocab[\"<pad>\"]\n",
    "PAD_TGT = tgt_vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "232efeea-8f3b-4ae8-bce5-202f76473b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "encoder = Encoder(input_vocab_size=len(src_vocab), embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM)\n",
    "decoder = Decoder(output_vocab_size=len(tgt_vocab), \n",
    "                  embed_dim=EMBED_DIM, \n",
    "                  hidden_dim=HIDDEN_DIM, \n",
    "                  enc_dim=HIDDEN_DIM*2)\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c99476cb-066a-41ca-8770-83bb892e8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function (ignores the padding tokens in the targets)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TGT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d6e45b6-c728-4fba-84c1-0e72430a2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label smoothing\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TGT, label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad2b05f6-1553-4151-8c65-c7e9a9cb35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizaer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60e92bc1-e096-459d-a9cd-dc1f96e8a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(46045, 256)\n",
      "    (gru): GRU(256, 256, batch_first=True, bidirectional=True)\n",
      "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(25767, 256)\n",
      "    (attention): LuongAttention(\n",
      "      (attn): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (ctx_proj): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (gru): GRU(512, 256, batch_first=True)\n",
      "    (fc_out): Linear(in_features=512, out_features=25767, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640e908-38f7-4605-8b9e-e4dcc0a94bef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4efe80c8-7c26-4802-8712-d6c579da7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(logits, targets, pad_idx):\n",
    "    # logits: (B,T,V), targets: (B,T)\n",
    "    with torch.no_grad():\n",
    "        preds = logits.argmax(-1)\n",
    "        mask = targets.ne(pad_idx)\n",
    "        correct = (preds.eq(targets) & mask).sum().item()\n",
    "        total = mask.sum().item()\n",
    "        return correct/max(total, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3b8cebb-18fe-47d5-9bbb-6dd0e9f12400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss, total_acc, steps=0.0,0.0,0\n",
    "    for src, tgt_in, tgt_out in tqdm(loader, desc=\"Train\" if train else \"Val\", leave=False):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt_in = tgt_in.to(DEVICE)\n",
    "        tgt_out = tgt_out.to(DEVICE)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        logits =model(src, tgt_in) #(B,T,V)\n",
    "        B, T, V = logits.shape\n",
    "        loss = criterion(logits.view(B*T,V),tgt_out.view(B*T))\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        acc = masked_accuracy(logits,tgt_out,PAD_TGT)\n",
    "        total_loss+=loss.item()\n",
    "        total_acc+=acc\n",
    "        steps+=1\n",
    "\n",
    "    return total_loss/steps, total_acc/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7488e14-ed31-48de-93d3-c542f1aa89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m best_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m run_epoch(val_loader, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loss\u001b[38;5;241m<\u001b[39mbest_val:\n",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(loader, train)\u001b[0m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT,V),tgt_out\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(),\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patience = 3\n",
    "wait = 0\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_acc = run_epoch(val_loader, train=False)\n",
    "\n",
    "    if val_loss<best_val:\n",
    "        best_val=val_loss\n",
    "        wait=0\n",
    "        best_state ={k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait +=1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train_loss {train_loss:.4f} acc {train_acc:.3f} | \"\n",
    "          f\"val_loss {val_loss:.4f} acc {val_acc:.3f}\")\n",
    "\n",
    "# restore best weights\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9e997-0e2c-4e16-9b8d-afbc7bc00539",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"nmt_luong_attn.pt\")\n",
    "print(\"Saved to nmt_luong_attn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe24c3d-4bd3-476e-999e-f4a5420faa1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inference (Greedy Decoding with Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc89cc-2edb-4ef0-877c-e887fbf28151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper to build a source tensor from Spanish text\n",
    "def prepare_src_sentence_es(sentence, src_vocab, device):\n",
    "    # no <start>/<end> on source\n",
    "    tokens = sentence.lower().strip().split()\n",
    "    idxs = [src_vocab.get(tok, src_vocab[\"<unk>\"]) for tok in tokens]\n",
    "    return torch.tensor(idxs,dtype=torch.long).unsqueeze(0).to(device) # (1,S)\n",
    "\n",
    "# index to word for target vocab\n",
    "tgt_idx2word = {idx: w for w, idx in tgt_vocab.items()}\n",
    "\n",
    "# greedy decode\n",
    "def translate_es_to_en(model, sentence_es, src_vocab, tgt_vocab, device, max_len= 40):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src = prepare_src_sentence_es(sentence_es, src_vocab, DEVICE)\n",
    "        encoder_outputs, hidden=model.encoder(src)\n",
    "\n",
    "        start_id=tgt_vocab[\"<start>\"]\n",
    "        end_id=tgt_vocab[\"<end>\"]\n",
    "        next_token = torch.tensor([[start_id]],dtype=torch.long,device=DEVICE)\n",
    "\n",
    "        decoded_ids = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden, attn = model.decoder.step(next_token, hidden, encoder_outputs)\n",
    "            next_id = int(logits[:, -1, :].argmax(dim=-1).item())\n",
    "            if next_id == end_id:\n",
    "                break\n",
    "            decoded_ids.append(next_id)\n",
    "            next_token = torch.tensor([[next_id]],dtype=torch.long,device=DEVICE)\n",
    "\n",
    "    words = [tgt_idx2word[i] for i in decoded_ids if tgt_idx2word[i] not in (\"<start>\", \"<end>\")]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8bb44-a1d9-4362-8326-1f273973555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    j = random.randint(0, len(spanish_sentences) - 1)\n",
    "    src_es = spanish_sentences[j]# Spanish source\n",
    "    ref_en = english_sentences[j]# English reference\n",
    "    pred_en = translate_es_to_en(model, src_es, src_vocab, tgt_vocab, DEVICE)\n",
    "\n",
    "    print(f\"ES: {src_es}\")\n",
    "    print(f\"PRED: {pred_en}\")\n",
    "    print(f\"REF: {ref_en}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abe853-46c8-457b-82bf-62c0f088e3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
