{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6 - Exercise 2\n",
    "### Daniel Mehta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('raw_partner_headlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Agilent Technologies Announces Pricing of $5……...</td>\n",
       "      <td>http://www.gurufocus.com/news/1153187/agilent-...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>2020-06-01 00:00:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Agilent (A) Gears Up for Q2 Earnings: What's i...</td>\n",
       "      <td>http://www.zacks.com/stock/news/931205/agilent...</td>\n",
       "      <td>Zacks</td>\n",
       "      <td>2020-05-18 00:00:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>J.P. Morgan Asset Management Announces Liquida...</td>\n",
       "      <td>http://www.gurufocus.com/news/1138923/jp-morga...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>2020-05-15 00:00:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Pershing Square Capital Management, L.P. Buys ...</td>\n",
       "      <td>http://www.gurufocus.com/news/1138704/pershing...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>2020-05-15 00:00:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Agilent Awards Trilogy Sciences with a Golden ...</td>\n",
       "      <td>http://www.gurufocus.com/news/1134012/agilent-...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>2020-05-12 00:00:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           headline  \\\n",
       "0           2  Agilent Technologies Announces Pricing of $5……...   \n",
       "1           3  Agilent (A) Gears Up for Q2 Earnings: What's i...   \n",
       "2           4  J.P. Morgan Asset Management Announces Liquida...   \n",
       "3           5  Pershing Square Capital Management, L.P. Buys ...   \n",
       "4           6  Agilent Awards Trilogy Sciences with a Golden ...   \n",
       "\n",
       "                                                 url  publisher  \\\n",
       "0  http://www.gurufocus.com/news/1153187/agilent-...  GuruFocus   \n",
       "1  http://www.zacks.com/stock/news/931205/agilent...      Zacks   \n",
       "2  http://www.gurufocus.com/news/1138923/jp-morga...  GuruFocus   \n",
       "3  http://www.gurufocus.com/news/1138704/pershing...  GuruFocus   \n",
       "4  http://www.gurufocus.com/news/1134012/agilent-...  GuruFocus   \n",
       "\n",
       "                  date stock  \n",
       "0  2020-06-01 00:00:00     A  \n",
       "1  2020-05-18 00:00:00     A  \n",
       "2  2020-05-15 00:00:00     A  \n",
       "3  2020-05-15 00:00:00     A  \n",
       "4  2020-05-12 00:00:00     A  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1845559\n"
     ]
    }
   ],
   "source": [
    "news = []\n",
    "for i, j in df.iterrows():\n",
    "    news.append(j['headline'])\n",
    "    \n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Agilent Technologies Announces Pricing of $5…… Million of Senior Notes']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1845559"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[:109233]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109233"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = os.path.join(os.getcwd(), 'finance_news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'w') as f:\n",
    "    f.write('\\n'.join(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
    "    \"\"\"\n",
    "    Preprocess Text Data\n",
    "    \"\"\"\n",
    "    text = load_data(dataset_path)\n",
    "    \n",
    "    # Ignore notice, since we don't use it for analysing the data\n",
    "    text = text[81:]\n",
    "\n",
    "    token_dict = token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))\n",
    "\n",
    "\n",
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'finance_news.txt'\n",
    "text = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 58299\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 9.44242124632666\n",
      "\n",
      "The lines 0 to 10:\n",
      "Agilent Technologies Announces Pricing of $5…… Million of Senior Notes\n",
      "Agilent (A) Gears Up for Q2 Earnings: What's in the Cards?\n",
      "J.P. Morgan Asset Management Announces Liquidation of Six Exchange-Traded Funds\n",
      "Pershing Square Capital Management, L.P. Buys Agilent Technologies Inc, The Howard Hughes Corp, ...\n",
      "Agilent Awards Trilogy Sciences with a Golden Ticket at LabCentral\n",
      "Agilent Technologies Inc (A) CEO and President Michael R. Mcmullen Sold $–.4 million of Shares\n",
      "' Stocks Growing Their Earnings Fast\n",
      "Cypress Asset Management Inc Buys Verizon Communications Inc, United Parcel Service Inc, ...\n",
      "Hendley & Co Inc Buys American Electric Power Co Inc, Agilent Technologies Inc, Paychex ...\n",
      "Teacher Retirement System Of Texas Buys Hologic Inc, Vanguard Total Stock Market, Agilent ...\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (0, 10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    word_count = Counter(text)\n",
    "    sorted_vocab = sorted(word_count, key = word_count.get, reverse=True)\n",
    "    int_to_vocab = {ii:word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = dict()\n",
    "    token['.'] = '<PERIOD>'\n",
    "    token[','] = '<COMMA>'\n",
    "    token['\"'] = 'QUOTATION_MARK'\n",
    "    token[';'] = 'SEMICOLON'\n",
    "    token['!'] = 'EXCLAIMATION_MARK'\n",
    "    token['?'] = 'QUESTION_MARK'\n",
    "    token['('] = 'LEFT_PAREN'\n",
    "    token[')'] = 'RIGHT_PAREN'\n",
    "    token['-'] = 'QUESTION_MARK'\n",
    "    token['\\n'] = 'NEW_LINE'\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    n_batches = len(words)//batch_size\n",
    "    x, y = [], []\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for ii in range(0, len(words)-sequence_length):\n",
    "        i_end = ii+sequence_length        \n",
    "        batch_x = words[ii:ii+sequence_length]\n",
    "        x.append(batch_x)\n",
    "        batch_y = words[i_end]\n",
    "        y.append(batch_y)\n",
    "    \n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "        \n",
    "    \n",
    "    # return a dataloader\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[35, 36, 37, 38, 39],\n",
      "        [19, 20, 21, 22, 23],\n",
      "        [17, 18, 19, 20, 21],\n",
      "        [ 7,  8,  9, 10, 11],\n",
      "        [ 3,  4,  5,  6,  7],\n",
      "        [23, 24, 25, 26, 27],\n",
      "        [18, 19, 20, 21, 22],\n",
      "        [31, 32, 33, 34, 35],\n",
      "        [39, 40, 41, 42, 43],\n",
      "        [38, 39, 40, 41, 42]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([40, 24, 22, 12,  8, 28, 23, 36, 44, 43])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = next(data_iter)  # using built in next() function instead of .next() method for iterators\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        # TODO: Implement function\n",
    "        \n",
    "        # define embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # define lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # set class variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # define model layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function   \n",
    "        batch_size = x.size(0)\n",
    "        x=x.long()\n",
    "        \n",
    "        # embedding and lstm_out \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack up lstm layers\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout, fc layer and final sigmoid layer\n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        # reshaping out layer to batch_size * seq_length * output_size\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        \n",
    "        # return last batch\n",
    "        out = out[:, -1]\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # create 2 new zero tensors of size n_layers * batch_size * hidden_dim\n",
    "        weights = next(self.parameters()).data\n",
    "        if(train_on_gpu):\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), \n",
    "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        rnn.cuda()\n",
    "    \n",
    "    # creating variables for hidden state to prevent back-propagation\n",
    "    # of historical states \n",
    "    h = tuple([each.data for each in hidden])\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    # move inputs, targets to GPU \n",
    "    inputs, targets = inp.cuda(), target.cuda()\n",
    "    \n",
    "    output, h = rnn(inputs, h)\n",
    "    \n",
    "    loss = criterion(output, targets)\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 10  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 250\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epoch(s)...\n",
      "Epoch:    1/10    Loss: 6.993944655418396\n",
      "\n",
      "Epoch:    1/10    Loss: 6.1529925899505615\n",
      "\n",
      "Epoch:    1/10    Loss: 5.768744532585144\n",
      "\n",
      "Epoch:    1/10    Loss: 5.521613512992859\n",
      "\n",
      "Epoch:    1/10    Loss: 5.35321697807312\n",
      "\n",
      "Epoch:    1/10    Loss: 5.216994276046753\n",
      "\n",
      "Epoch:    1/10    Loss: 5.087665128707886\n",
      "\n",
      "Epoch:    1/10    Loss: 4.985864153862\n",
      "\n",
      "Epoch:    1/10    Loss: 4.9125787000656125\n",
      "\n",
      "Epoch:    1/10    Loss: 4.819999813556671\n",
      "\n",
      "Epoch:    1/10    Loss: 4.785653531551361\n",
      "\n",
      "Epoch:    1/10    Loss: 4.675312551498413\n",
      "\n",
      "Epoch:    1/10    Loss: 4.630319984436035\n",
      "\n",
      "Epoch:    1/10    Loss: 4.589827892303467\n",
      "\n",
      "Epoch:    1/10    Loss: 4.520943288326263\n",
      "\n",
      "Epoch:    1/10    Loss: 4.504897451877594\n",
      "\n",
      "Epoch:    1/10    Loss: 4.460526263713836\n",
      "\n",
      "Epoch:    1/10    Loss: 4.418950723171234\n",
      "\n",
      "Epoch:    1/10    Loss: 4.3891025171279905\n",
      "\n",
      "Epoch:    1/10    Loss: 4.357330962657929\n",
      "\n",
      "Epoch:    2/10    Loss: 4.189208025261476\n",
      "\n",
      "Epoch:    2/10    Loss: 4.10013459777832\n",
      "\n",
      "Epoch:    2/10    Loss: 4.077091027259827\n",
      "\n",
      "Epoch:    2/10    Loss: 4.055426326274872\n",
      "\n",
      "Epoch:    2/10    Loss: 4.051941301345825\n",
      "\n",
      "Epoch:    2/10    Loss: 4.027236931800842\n",
      "\n",
      "Epoch:    2/10    Loss: 4.038839217185974\n",
      "\n",
      "Epoch:    2/10    Loss: 4.02114538192749\n",
      "\n",
      "Epoch:    2/10    Loss: 4.002659096717834\n",
      "\n",
      "Epoch:    2/10    Loss: 3.992010589599609\n",
      "\n",
      "Epoch:    2/10    Loss: 3.9921094298362734\n",
      "\n",
      "Epoch:    2/10    Loss: 3.983956072330475\n",
      "\n",
      "Epoch:    2/10    Loss: 4.011128291130066\n",
      "\n",
      "Epoch:    2/10    Loss: 3.957461054801941\n",
      "\n",
      "Epoch:    2/10    Loss: 3.9485399923324587\n",
      "\n",
      "Epoch:    2/10    Loss: 3.942023030757904\n",
      "\n",
      "Epoch:    2/10    Loss: 3.957240201473236\n",
      "\n",
      "Epoch:    2/10    Loss: 3.9657868704795836\n",
      "\n",
      "Epoch:    2/10    Loss: 3.9279524698257444\n",
      "\n",
      "Epoch:    2/10    Loss: 3.918103006362915\n",
      "\n",
      "Epoch:    3/10    Loss: 3.757539758513031\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6790250129699706\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6784533581733703\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6833298907279968\n",
      "\n",
      "Epoch:    3/10    Loss: 3.7099530897140505\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6990194683074953\n",
      "\n",
      "Epoch:    3/10    Loss: 3.691033864021301\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6963361291885377\n",
      "\n",
      "Epoch:    3/10    Loss: 3.696784559249878\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6761943950653078\n",
      "\n",
      "Epoch:    3/10    Loss: 3.690470257282257\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6814261226654055\n",
      "\n",
      "Epoch:    3/10    Loss: 3.690713716983795\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6857989077568054\n",
      "\n",
      "Epoch:    3/10    Loss: 3.716593376159668\n",
      "\n",
      "Epoch:    3/10    Loss: 3.7060396580696104\n",
      "\n",
      "Epoch:    3/10    Loss: 3.7227933435440064\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6932522168159485\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6860303072929383\n",
      "\n",
      "Epoch:    3/10    Loss: 3.7337316503524782\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5524714210640793\n",
      "\n",
      "Epoch:    4/10    Loss: 3.4709064354896544\n",
      "\n",
      "Epoch:    4/10    Loss: 3.4818826422691345\n",
      "\n",
      "Epoch:    4/10    Loss: 3.4830974969863893\n",
      "\n",
      "Epoch:    4/10    Loss: 3.4983982071876527\n",
      "\n",
      "Epoch:    4/10    Loss: 3.491863907337189\n",
      "\n",
      "Epoch:    4/10    Loss: 3.529422034263611\n",
      "\n",
      "Epoch:    4/10    Loss: 3.504852892398834\n",
      "\n",
      "Epoch:    4/10    Loss: 3.516963713169098\n",
      "\n",
      "Epoch:    4/10    Loss: 3.505481177806854\n",
      "\n",
      "Epoch:    4/10    Loss: 3.50170051240921\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5350561804771425\n",
      "\n",
      "Epoch:    4/10    Loss: 3.519514736175537\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5131672954559328\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5451217436790468\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5367036352157593\n",
      "\n",
      "Epoch:    4/10    Loss: 3.559509958267212\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5625045561790465\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5684971590042114\n",
      "\n",
      "Epoch:    4/10    Loss: 3.576594874382019\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4195049056837616\n",
      "\n",
      "Epoch:    5/10    Loss: 3.3419065127372742\n",
      "\n",
      "Epoch:    5/10    Loss: 3.374010527610779\n",
      "\n",
      "Epoch:    5/10    Loss: 3.3253595323562624\n",
      "\n",
      "Epoch:    5/10    Loss: 3.362663249492645\n",
      "\n",
      "Epoch:    5/10    Loss: 3.3648437700271607\n",
      "\n",
      "Epoch:    5/10    Loss: 3.3824815311431884\n",
      "\n",
      "Epoch:    5/10    Loss: 3.397530748844147\n",
      "\n",
      "Epoch:    5/10    Loss: 3.360007179737091\n",
      "\n",
      "Epoch:    5/10    Loss: 3.385705491542816\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4034339265823363\n",
      "\n",
      "Epoch:    5/10    Loss: 3.394639880180359\n",
      "\n",
      "Epoch:    5/10    Loss: 3.434655624389648\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4195758385658266\n",
      "\n",
      "Epoch:    5/10    Loss: 3.419886960029602\n",
      "\n",
      "Epoch:    5/10    Loss: 3.432862590789795\n",
      "\n",
      "Epoch:    5/10    Loss: 3.437755522251129\n",
      "\n",
      "Epoch:    5/10    Loss: 3.442976296424866\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4706301856040955\n",
      "\n",
      "Epoch:    5/10    Loss: 3.45311723279953\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3147154386203703\n",
      "\n",
      "Epoch:    6/10    Loss: 3.2438850336074827\n",
      "\n",
      "Epoch:    6/10    Loss: 3.2753872771263124\n",
      "\n",
      "Epoch:    6/10    Loss: 3.2648511233329773\n",
      "\n",
      "Epoch:    6/10    Loss: 3.2717924079895018\n",
      "\n",
      "Epoch:    6/10    Loss: 3.286987283706665\n",
      "\n",
      "Epoch:    6/10    Loss: 3.282788242340088\n",
      "\n",
      "Epoch:    6/10    Loss: 3.320651725769043\n",
      "\n",
      "Epoch:    6/10    Loss: 3.283897407054901\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3035778160095215\n",
      "\n",
      "Epoch:    6/10    Loss: 3.2987783522605896\n",
      "\n",
      "Epoch:    6/10    Loss: 3.308003444671631\n",
      "\n",
      "Epoch:    6/10    Loss: 3.325775638580322\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3243118500709534\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3516270656585694\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3465147891044618\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3556078910827636\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3375739755630494\n",
      "\n",
      "Epoch:    6/10    Loss: 3.3597601532936094\n",
      "\n",
      "Epoch:    6/10    Loss: 3.369690434932709\n",
      "\n",
      "Epoch:    7/10    Loss: 3.2324840762618225\n",
      "\n",
      "Epoch:    7/10    Loss: 3.1617148151397707\n",
      "\n",
      "Epoch:    7/10    Loss: 3.1622398438453674\n",
      "\n",
      "Epoch:    7/10    Loss: 3.183980472564697\n",
      "\n",
      "Epoch:    7/10    Loss: 3.186767496585846\n",
      "\n",
      "Epoch:    7/10    Loss: 3.210755100727081\n",
      "\n",
      "Epoch:    7/10    Loss: 3.207624593257904\n",
      "\n",
      "Epoch:    7/10    Loss: 3.2116354274749757\n",
      "\n",
      "Epoch:    7/10    Loss: 3.2185562801361085\n",
      "\n",
      "Epoch:    7/10    Loss: 3.242059384346008\n",
      "\n",
      "Epoch:    7/10    Loss: 3.2538273367881776\n",
      "\n",
      "Epoch:    7/10    Loss: 3.238495846271515\n",
      "\n",
      "Epoch:    7/10    Loss: 3.2427316989898682\n",
      "\n",
      "Epoch:    7/10    Loss: 3.2580183119773864\n",
      "\n",
      "Epoch:    7/10    Loss: 3.2772063665390014\n",
      "\n",
      "Epoch:    7/10    Loss: 3.296006287574768\n",
      "\n",
      "Epoch:    7/10    Loss: 3.306935854911804\n",
      "\n",
      "Epoch:    7/10    Loss: 3.2765626463890074\n",
      "\n",
      "Epoch:    7/10    Loss: 3.2786044850349425\n",
      "\n",
      "Epoch:    7/10    Loss: 3.3104314770698546\n",
      "\n",
      "Epoch:    8/10    Loss: 3.1804552084290636\n",
      "\n",
      "Epoch:    8/10    Loss: 3.103109248638153\n",
      "\n",
      "Epoch:    8/10    Loss: 3.1078985562324526\n",
      "\n",
      "Epoch:    8/10    Loss: 3.12465931224823\n",
      "\n",
      "Epoch:    8/10    Loss: 3.1350995082855224\n",
      "\n",
      "Epoch:    8/10    Loss: 3.1685340185165405\n",
      "\n",
      "Epoch:    8/10    Loss: 3.1492422065734864\n",
      "\n",
      "Epoch:    8/10    Loss: 3.151209388256073\n",
      "\n",
      "Epoch:    8/10    Loss: 3.161052990436554\n",
      "\n",
      "Epoch:    8/10    Loss: 3.1811849036216735\n",
      "\n",
      "Epoch:    8/10    Loss: 3.174163818359375\n",
      "\n",
      "Epoch:    8/10    Loss: 3.187234085083008\n",
      "\n",
      "Epoch:    8/10    Loss: 3.1788876676559448\n",
      "\n",
      "Epoch:    8/10    Loss: 3.204271884918213\n",
      "\n",
      "Epoch:    8/10    Loss: 3.2245695185661316\n",
      "\n",
      "Epoch:    8/10    Loss: 3.219024398326874\n",
      "\n",
      "Epoch:    8/10    Loss: 3.1868942914009093\n",
      "\n",
      "Epoch:    8/10    Loss: 3.2357496848106386\n",
      "\n",
      "Epoch:    8/10    Loss: 3.2361267704963685\n",
      "\n",
      "Epoch:    8/10    Loss: 3.2235345001220703\n",
      "\n",
      "Epoch:    9/10    Loss: 3.093372000185407\n",
      "\n",
      "Epoch:    9/10    Loss: 3.0271588859558105\n",
      "\n",
      "Epoch:    9/10    Loss: 3.045473005771637\n",
      "\n",
      "Epoch:    9/10    Loss: 3.046944664001465\n",
      "\n",
      "Epoch:    9/10    Loss: 3.055320095062256\n",
      "\n",
      "Epoch:    9/10    Loss: 3.094630542755127\n",
      "\n",
      "Epoch:    9/10    Loss: 3.1152786622047426\n",
      "\n",
      "Epoch:    9/10    Loss: 3.0821052112579346\n",
      "\n",
      "Epoch:    9/10    Loss: 3.1092875213623046\n",
      "\n",
      "Epoch:    9/10    Loss: 3.102768180847168\n",
      "\n",
      "Epoch:    9/10    Loss: 3.1256769275665284\n",
      "\n",
      "Epoch:    9/10    Loss: 3.124481111526489\n",
      "\n",
      "Epoch:    9/10    Loss: 3.1167268490791322\n",
      "\n",
      "Epoch:    9/10    Loss: 3.175630630016327\n",
      "\n",
      "Epoch:    9/10    Loss: 3.1422842245101927\n",
      "\n",
      "Epoch:    9/10    Loss: 3.161958396434784\n",
      "\n",
      "Epoch:    9/10    Loss: 3.194431569099426\n",
      "\n",
      "Epoch:    9/10    Loss: 3.1797687463760376\n",
      "\n",
      "Epoch:    9/10    Loss: 3.174331483364105\n",
      "\n",
      "Epoch:    9/10    Loss: 3.2016622443199156\n",
      "\n",
      "Epoch:   10/10    Loss: 3.0666787881210578\n",
      "\n",
      "Epoch:   10/10    Loss: 2.975358413219452\n",
      "\n",
      "Epoch:   10/10    Loss: 3.0065314846038818\n",
      "\n",
      "Epoch:   10/10    Loss: 3.0024210953712465\n",
      "\n",
      "Epoch:   10/10    Loss: 3.0132005343437194\n",
      "\n",
      "Epoch:   10/10    Loss: 3.0244974813461303\n",
      "\n",
      "Epoch:   10/10    Loss: 3.041077748298645\n",
      "\n",
      "Epoch:   10/10    Loss: 3.0541318411827088\n",
      "\n",
      "Epoch:   10/10    Loss: 3.0660813546180723\n",
      "\n",
      "Epoch:   10/10    Loss: 3.072396270751953\n",
      "\n",
      "Epoch:   10/10    Loss: 3.0773998289108278\n",
      "\n",
      "Epoch:   10/10    Loss: 3.1026220908164976\n",
      "\n",
      "Epoch:   10/10    Loss: 3.1100303130149842\n",
      "\n",
      "Epoch:   10/10    Loss: 3.098790253162384\n",
      "\n",
      "Epoch:   10/10    Loss: 3.098059878349304\n",
      "\n",
      "Epoch:   10/10    Loss: 3.133239384651184\n",
      "\n",
      "Epoch:   10/10    Loss: 3.114422559738159\n",
      "\n",
      "Epoch:   10/10    Loss: 3.1404697575569154\n",
      "\n",
      "Epoch:   10/10    Loss: 3.1519603643417358\n",
      "\n",
      "Epoch:   10/10    Loss: 3.1421021361351014\n",
      "\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "save_model('trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
    "trained_rnn = torch.load('trained_rnn.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesla\n",
      "american tower(amt) gains from market anticipated cagr of —4%...\n",
      "global organic photovoltaics market —…—… global industry analysis, industry share...\n",
      "the vetr community has downgraded $amx to 2? stars\n",
      "the vetr community has downgraded $anf to\n"
     ]
    }
   ],
   "source": [
    "gen_length = 50 # modify the length to your preference\n",
    "prime_words = ['tesla'] # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "for prime_word in prime_words:\n",
    "    pad_word = SPECIAL_WORDS['PADDING']\n",
    "    generated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "    print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- Used 100k+ financial news headlines as training data\n",
    "- Preprocessed text and converted it to integer sequences\n",
    "- Built and trained an LSTM model for word level text generation\n",
    "- Generated sample headlines using topk sampling"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 754810,
     "sourceId": 1304644,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29987,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (hackathon)",
   "language": "python",
   "name": "hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
